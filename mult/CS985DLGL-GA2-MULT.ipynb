{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTV40543CWX9"
   },
   "source": [
    "# Multi-Classification Problem\n",
    "## CS985 Deep Learning Group L\n",
    "<p>Ian Richardson 202074007</p>\n",
    "<p>Fraser Bayne 202053049</p> \n",
    "<p>Slav Ivanov 201645797</p> \n",
    "<p>Lora Kiosseva 202082329</p> \n",
    "\n",
    "---\n",
    "In short, our group implemented <b>5 models:</b> \n",
    "- A standard ensamble method, Random Forest,\n",
    "- A baseline Neural Network\n",
    "- An LSTM-based model \n",
    "- A BERT-based model\n",
    "- A Word2Vec-based model\n",
    "\n",
    "Our main finding is that the performance of the models is heavily based on the data augmentation. Models such as the BERT-based one performs very well, achieving Kaggle scores of ~0.92."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrYh9QR8wRF9"
   },
   "source": [
    "# Method\n",
    "\n",
    "The process we used for data processing includes a number of steps. After the .csv files are loaded, all labels with less than 1000 occurances are dropped. After examining the **label** column, we saw that the main labels(the ones that contain a single 1 in them, i.e **000000001**) were the ones that were predominantly represented. \n",
    "\n",
    "Dropping everything below 1000 occurances fully covers the 9 main labels and, respectively, the 9 main categories of document. The original value at which the cutoff happened was 100. That left in 29 unique labels which reduced the trianing accuracy and loss score, as these other 20 labels were relatively under-represented and had less occurances to train on.\n",
    "\n",
    "The next step involves dropping the columns named\n",
    "**'docid', 'publication_date', 'category', 'country_code', 'country_name', 'sector', 'value'.** From our inspection, these colums did not seem to add much to the data and we decided to remove them. Most of them are just unique identifiers within the dataframe, whereas others(such as country_name) simply had the same value inside. Others (value) had NaN values in them and it was not worth trying to fill them in.\n",
    "\n",
    "Subsequently, the remaining columns are combined within a single String column called 'data'. **This way all the data for the particular label can be seen as a single string that can be converted to a single vector, or passed into BERT more easily.**\n",
    "\n",
    "---\n",
    "\n",
    "The dataset is split up into X and y, **where X is the 'data' column and y the 'label'.** \n",
    "\n",
    "- The y dataset is then transformed by a LabelEncoder. This would enable having the final activation for the NNs to be **softmax**. This approach is very similar to the one used for the **Fashion-MNIST task described in the book Hands-On Machine Learning with Scikit-Learn, Keras and Tensorflow[1]**\n",
    "- The X dataset is transformed into numeric vectors through a CountVectorizer. This data would be used in all models apart from the BERT and Word2Vec based ones. \n",
    "\n",
    "This is then split up into testing and training sets through the use of train_test_split. The training set is split up into a **training and validation set**, to avoid overfitting. \n",
    "\n",
    "---\n",
    "\n",
    "## **EVERY NN USES THE SAME FINAL LAYER FOR CSV GENERATION AND TESTING CONSISTENCY**\n",
    "**The final layer contains neurons equal to however many unique labels were produced by the LabelEncoder(10 when the cutoff for labels is 1000 occurances, 29 when it is 100). The activation function is softmax.**\n",
    "\n",
    "## Baseline Model - function: **rnd_for()**\n",
    "The baseline model of choice was a **scikit-learn ensamble method model - Random Forest**. It was chosen as it is considered to be one of the most versatile Machine Learning models. Despite it being simpler than a Neural Network(NN), it still manages to perform quite well in multi-class classification tasks, such as this one. \n",
    "\n",
    "## Baseline NN - class: **BaselineNN()**\n",
    "\n",
    "The baseline NN of choice is a fully-connected, feedforward Neural Network, consisting of 3 layers of 300 neurons each. Each layer's activation function is ReLU. These 3 layers are followed by the output layer.\n",
    "\n",
    "## LSTM Model - class: **LSTMModel()**\n",
    "The LSTM was chosen due to the fact that the data we were dealing with was in a text format. RNNs in general perform quite well at task like these and the LSTM being a better version of them was a natural choice. This LSTM model consists of 2 LSTM layers, followed by a Dense layer, that is ultimately followed by the output layer.\n",
    "\n",
    "## BERT Model - function: **bertholomew()**\n",
    "This model utilises the fine-tuning of a BERT model[2]. The preprocessing unit and encoder are obtained through tfhub. The BERT model of choice is **bert_multi_cased_L-12_H-768_A-12/3** and its respective encoder. We chose it as it was case-sensitive, multilingual, and it had multiple large layers. The BERT layers are then followed by a combination of 3 Dense layers and Dropouts, ending in the final output layer.\n",
    "\n",
    "\n",
    "## Word2Vec - class: **Word2Vec()**\n",
    "\n",
    "\n",
    "This model uses the text vectorization layer to normalize, split, and map strings from the dataset to integers. output_sequence_length length is used to pad all samples to same length. Sequence_length is set 10 as that's roughly how many words each row in the data has. If a row has less words, it is padded with 0s.\n",
    "\n",
    "Once all the data has been integer encoded, skip-gram pairs with negative sampling for a list of sequences (int-encoded sentences) is generated based on window size, number of negative samples and vocabulary size as per [3] This gives us targets, contexts and labels, which word2vec is trained on. \n",
    "\n",
    "Once word2vec is trained, it generates a matrix of wights (embedding matrix), which can be passed to an embedding layer of a deep learning model. The output of this Embedding layer is a 2D matrix with a word vector for each word id in the input sequence (Sequence length x Vector size). This is followed by two layers of Bidirectional LSTM to model the order of words in a sequence in both directions. Two final dense layers that predict the probability for each category. The model used is lrgely inspired by [4].\n",
    "\n",
    "## Predict - function: **generate_csv**\n",
    "\n",
    "Finally, this function will take the trained model and get a prediction for the test dataset, writting it out to a CSV that can be submitted to Kaggle. From the values predicted we pick the one with the highest value predicted and get the index of the label at that position. Using this index, we can get the specific label from the list of unique labels and thus we have our end prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## Training - function: **trainModel(model, xm_train, ym_train, xm_val, ym_val)**\n",
    "\n",
    "Training was done with 5 to 10 epochs of training for the models, due to the fact that some models take considerably longer than others to train.  Sparse-categorical-crossentropy was used as the loss, as we had a single output neuron per output label. Accuracy was used as the metric and stochastic gradient descent as the optimiser. Adam was used at first but it converged slower and to a less desirable loss fucntion score.\n",
    "\n",
    "Through experimentation with the models' hyperparameters, different values produced different results. The results gathered did not improve when changes were made, save from increasing training times.\n",
    "\n",
    "---\n",
    "\n",
    "## Performance\n",
    "\n",
    "Due to RAM limitations and crashed, variables are deleted and garbage collected when training of a model finishes. Due to this, the Random Forest had to be trained on only a 1000 row subset of the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oZSslCt7a7CZ"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D3Yt84Xxa8NY",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install -q tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "FWlcDEuQ_x-U"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-715e02ba50df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_text\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFo_1AVG_5Fj"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./dataset/german-contracts-train.csv', dtype={\n",
    "        \"docid\":str,\n",
    "        \"publication_date\":str,\n",
    "        \"contract_type\":str,\n",
    "        \"nature_of_contract\":str,\n",
    "        \"country_code\":str,\n",
    "        \"country_name\":str,\n",
    "        \"sector\":str,\n",
    "        \"category\":str,\n",
    "        \"value\":float,\n",
    "        \"title\":str,\n",
    "        \"description\":str,\n",
    "        \"awarding_authority\":str,\n",
    "        \"complete_entry\":str,\n",
    "        \"label\":str})\n",
    "\n",
    "test_data = pd.read_csv('./dataset/german-contracts-test.csv', dtype={\n",
    "        \"docid\":str,\n",
    "        \"publication_date\":str,\n",
    "        \"contract_type\":str,\n",
    "        \"nature_of_contract\":str,\n",
    "        \"country_code\":str,\n",
    "        \"country_name\":str,\n",
    "        \"sector\":str,\n",
    "        \"category\":str,\n",
    "        \"value\":float,\n",
    "        \"title\":str,\n",
    "        \"description\":str,\n",
    "        \"awarding_authority\":str,\n",
    "        \"complete_entry\":str,\n",
    "        \"label\":str})\n",
    "id_column = np.array(test_data[\"docid\"]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_yS95KVx7Io"
   },
   "outputs": [],
   "source": [
    "data = data.groupby('label').filter(lambda x : len(x)>=1000)\n",
    "\n",
    "# DROP COLUMNS\n",
    "data = data.drop(columns=['docid', 'publication_date', 'category', 'country_code', 'country_name', 'sector', 'value'])\n",
    "test_data = test_data.drop(columns=['docid', 'publication_date', 'country_code', 'country_name', 'sector', 'value'])\n",
    "\n",
    "data = data.dropna()\n",
    "test_data = test_data.fillna('nan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v6nfxdZ9T14y"
   },
   "outputs": [],
   "source": [
    "data['data'] = data['contract_type'] + ' ' + data['nature_of_contract'] + ' ' + data['title'] + ' '\n",
    "+ data['description'] + ' ' + data['awarding_authority']\n",
    "data = data.drop(columns=['contract_type', 'nature_of_contract', 'title', 'description', 'awarding_authority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMY8tfbST2SJ"
   },
   "outputs": [],
   "source": [
    "test_data['data'] = test_data['contract_type'] + ' ' + test_data['nature_of_contract'] + ' ' + test_data['title'] + ' '\n",
    "+ test_data['description'] + ' ' + test_data['awarding_authority']\n",
    "test_data = test_data.drop(columns=['contract_type', 'nature_of_contract', 'title', 'description', 'awarding_authority'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ifPXuLffLrNY"
   },
   "outputs": [],
   "source": [
    "def vectorize_data(data, test_data):\n",
    "  vectorizer = CountVectorizer(min_df=0, lowercase=False, analyzer='word')\n",
    "  vectorizer.fit(data)\n",
    "  return vectorizer.transform(data).toarray().astype(np.float32), vectorizer.transform(test_data).toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkHZvC-XMFHP"
   },
   "outputs": [],
   "source": [
    "def label_encode(labels):\n",
    "  le = LabelEncoder()\n",
    "  le.fit(labels)\n",
    "  return np.unique(labels), le.transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTzt8XtvMnMA"
   },
   "outputs": [],
   "source": [
    "# Get everything except what we want to predict\n",
    "def prepare_baseline_data():\n",
    "  X, X_forreal = vectorize_data(np.array(data['data']), np.array(test_data['data']))\n",
    "  # Column we want to predict\n",
    "  y_uniques, y = label_encode(np.array(data['label']))\n",
    "\n",
    "  return X, X_forreal, y, y_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAOmK1U0cQuE"
   },
   "outputs": [],
   "source": [
    "def prepare_bert_data():\n",
    "  # Get everything except what we want to predict\n",
    "  X = np.array(data['data'])\n",
    "  X_forreal = np.array(test_data['data'])\n",
    "  # Column we want to predict\n",
    "  y_uniques, y = label_encode(np.array(data['label']))\n",
    "\n",
    "  return X, X_forreal, y, y_uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HKSlfyKFVplv"
   },
   "outputs": [],
   "source": [
    "def rnd_for():\n",
    "    cutoff = 1000\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train_full[:cutoff, :], y_train_full[:cutoff])\n",
    "    y_pred = model.predict(X_test[:cutoff, :])\n",
    "    print('> Random Forest Classifier', model.score(X_test[:cutoff, :], y_test[:cutoff]),\n",
    "          '- F1', f1_score(y_test[:cutoff], y_pred, average='macro'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UgYalOvNtNkT"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3421150be53d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mBaselineNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# handles standard args (e.g., name)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "class BaselineNN(keras.models.Model):\n",
    "    def __init__(self, units=300, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden3 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(len(y_uniques), activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        hidden1 = self.hidden1(inputs)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        hidden3 = self.hidden3(hidden2)\n",
    "        main_output = self.main_output(hidden3)\n",
    "        return main_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aNszZwtfdxQn"
   },
   "outputs": [],
   "source": [
    " class LSTMModel(keras.models.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.LSTM(256, return_sequences=True)\n",
    "        self.hidden2 = keras.layers.Dropout(0.3)\n",
    "        self.hidden3 = keras.layers.LSTM(128)\n",
    "        self.hidden4 = keras.layers.Dense(256)\n",
    "        self.hidden5 = keras.layers.Dropout(0.3)\n",
    "        self.main_output = keras.layers.Dense(len(y_uniques), activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        hidden1 = self.hidden1(inputs)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        hidden3 = self.hidden3(hidden2)\n",
    "        hidden4 = self.hidden4(hidden3)\n",
    "        hidden5 = self.hidden5(hidden4)\n",
    "        main_output = self.main_output(hidden5)\n",
    "        return main_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ctPOFNNieast"
   },
   "outputs": [],
   "source": [
    "def bertholomew():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='input')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(300, activation=\"relu\")(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(300, activation=\"relu\")(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(300, activation=\"relu\")(net)\n",
    "    net = tf.keras.layers.Dense(len(y_uniques), activation='softmax', name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "U-NSlS31u4Kr"
   },
   "outputs": [],
   "source": [
    "def trainModel(model, xm_train, ym_train, xm_val, ym_val):\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd', metrics=[\"accuracy\"])\n",
    "    b_size = 32\n",
    "    eps = 5\n",
    "\n",
    "    model.fit(xm_train, ym_train, batch_size=b_size, epochs=eps, validation_data=(xm_val, ym_val))\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    y_pred_classes = np.empty((0))\n",
    "    y_pred_final = []\n",
    "    y_test_final = []\n",
    "    for element in y_pred:\n",
    "      y_pred_classes = np.append(y_pred_classes, np.where(element == np.amax(element))[0][0])\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "      y_pred_final.append(y_uniques[y_pred_classes[i].astype(np.int64)])\n",
    "      y_test_final.append(y_uniques[y_test[i].astype(np.int64)])\n",
    "    \n",
    "    y_pred_final = np.array(y_pred_final)\n",
    "    y_test_final = np.array(y_test_final)\n",
    "    print(\"F1 SCORE: \", f1_score(y_test_final, y_pred_final, average=\"macro\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Ey7FgJ1lQ-l9"
   },
   "outputs": [],
   "source": [
    "def generate_csv(model, x, name):\n",
    "  y_pred = model.predict(x)\n",
    "  y_pred_final = []\n",
    "\n",
    "  if(name == \"sklearnrndfor\"):\n",
    "    for i in range(len(y_pred)):\n",
    "      y_pred_final.append(y_uniques[y_pred[i].astype(np.int64)])\n",
    "  else:\n",
    "    y_pred_classes = np.empty((0))\n",
    "    for element in y_pred:\n",
    "      y_pred_classes = np.append(y_pred_classes, np.where(element == np.amax(element))[0][0])\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "      y_pred_final.append(y_uniques[y_pred_classes[i].astype(np.int64)])\n",
    "  \n",
    "  y_pred_final = np.array(y_pred_final)\n",
    "  csv = np.concatenate((id_column, y_pred_final.reshape(-1,1)), axis=1)\n",
    "  csv = np.vstack((np.array([\"docid\",\"label\"]), csv))\n",
    "  np.savetxt(\"./csv/\" + name + \".csv\", csv, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-cTBw-WOcL6"
   },
   "source": [
    "# Results and Discussion\n",
    "- **(Accuracy: 0.877 - f1: 0.69 - Kaggle: 0.80487)** - rnd_for() \n",
    "- **(Accuracy: 0.949 - f1: 0.81 - Kaggle: 0.92204)** - BaselineNN()\n",
    "- **(Accuracy: 0.599 - f1: 0.07 - Kaggle: 0.55385)** - LSTMModel() \n",
    "- **(Accuracy: 0.978 - f1: 0.88 - Kaggle: 0.91853)** - bertholomew() \n",
    "- **(Accuracy: 0.689 - f1: 0.20 - Kaggle: 0.60257)** - word2vec \n",
    "\n",
    "---\n",
    "\n",
    "In general, our findings show that the BaselineNN utilising CountVectorizer and BERT are very-much so neck and neck, in terms of Kaggle performance and general classification power. \n",
    "\n",
    "Predicting labels directly through the use of a LabelEncoder is also another useful thing for us, as we do not have to combine the categories into the format that is requested. We simply get the index of the label that the Model is most confident is the predicted one(highest value amongst the output neurons). Using this index, we can get the specific label from the list of unique labels and thus we have our end prediction. **This approach is identical to the one seen in the Course AI book [1] for the Fashion-MNIST Data set(pages 294-295).**\n",
    "\n",
    "The featurs selected to be most important to us are:\n",
    "\n",
    "- contract_type\n",
    "- nature_of_contract\n",
    "- title\n",
    "- description\n",
    "- awarding_authority\n",
    "\n",
    "Models tend to predominantly fail due to Memory concerns, not enabling them to trian, hance the deletion of variables. Low number of epochs, poorly configured optimizers(very low learning rates), and using substes of the data, expectedly, cause the models to underperform.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4YMNwPDfmPU"
   },
   "source": [
    "# Summary and Recommendation\n",
    "\n",
    "All in all, the system built can support more models than the ones described here. In terms of recommendations, the process of combining columns and vectorizing them proved to be a very robust one and it produces satisfactory results. The vectors produced can surely be used in different and more complex models. The combination of the data columns increased the scores that the BERT model produces as well. Last but not least, dropping any and all labels that were under-represented proved to be a succesfull strategy that increased accuracy scores across all models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTiz88mvf4iP"
   },
   "source": [
    "# References\n",
    "\n",
    "1. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, Aurélien Géron **(Idea for final layer consisting a neuron for each class, softmax activation function)**\n",
    "\n",
    "2. https://www.tensorflow.org/tutorials/text/classify_text_with_bert - **BERT Model**\n",
    "\n",
    "3. https://www.tensorflow.org/tutorials/text/word2vec - **Word2Vec**\n",
    "\n",
    "4. https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 - **Word2Vec**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5XUy0VJsdYJa"
   },
   "source": [
    "# Training and Validating\n",
    "\n",
    "## Load in and split vectorized data\n",
    "\n",
    "This data will be used in the following models:\n",
    "- Random Forest\n",
    "- BaselineNN\n",
    "- LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsMzX6puc1ft",
    "outputId": "14213df8-959f-48cc-b63b-fb1c9d381312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X_forreal, y, y_uniques = prepare_baseline_data()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_valid, X_train = X_train_full[:4000], X_train_full[4000:]\n",
    "y_valid, y_train = y_train_full[:4000], y_train_full[4000:]\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdTP_RZ7kmkt"
   },
   "source": [
    "# This the training of the baseline ML Model - **rnd_for()**\n",
    "- This is run with only 1000 lines of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ihsqmlyV8es",
    "outputId": "a5e7a804-eb5d-43c7-d3cb-a709bf402ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Random Forest Classifier 0.877 - F1 0.6854916714523083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelSklearn = rnd_for()\n",
    "generate_csv(modelSklearn, X_forreal, \"sklearnrndfor\")\n",
    "del modelSklearn\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x65cpfYEksSH"
   },
   "source": [
    "# This the training of the baseline NN Model - **BaselineNN()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9Qe4Br_vZFg",
    "outputId": "797a78a3-2a3b-4466-8f71-fc6f1a7b2168"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2171/2171 [==============================] - 7s 3ms/step - loss: 1.2855 - accuracy: 0.6258 - val_loss: 0.7182 - val_accuracy: 0.7897\n",
      "Epoch 2/5\n",
      "2171/2171 [==============================] - 6s 3ms/step - loss: 0.6405 - accuracy: 0.8215 - val_loss: 0.4180 - val_accuracy: 0.8820\n",
      "Epoch 3/5\n",
      "2171/2171 [==============================] - 6s 3ms/step - loss: 0.3828 - accuracy: 0.8937 - val_loss: 0.2907 - val_accuracy: 0.9202\n",
      "Epoch 4/5\n",
      "2171/2171 [==============================] - 6s 3ms/step - loss: 0.2594 - accuracy: 0.9312 - val_loss: 0.2247 - val_accuracy: 0.9395\n",
      "Epoch 5/5\n",
      "2171/2171 [==============================] - 7s 3ms/step - loss: 0.2013 - accuracy: 0.9489 - val_loss: 0.1858 - val_accuracy: 0.9492\n",
      "F1 SCORE:  0.8111930078338488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3267"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelBase = trainModel(BaselineNN(), X_train, y_train, X_valid, y_valid)\n",
    "generate_csv(modelBase, X_forreal, \"base\")\n",
    "del modelBase\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3u7uVN7Ck2IG"
   },
   "source": [
    "# This the training of the LSTM Model - **LSTMModel()**\n",
    "\n",
    "- Data needs to first be reshaped to be used by this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByIc0qwpd-Uy",
    "outputId": "67f43271-cf54-43f1-cd1a-81d667cf7e20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2171/2171 [==============================] - 1207s 554ms/step - loss: 1.5627 - accuracy: 0.5912 - val_loss: 1.2295 - val_accuracy: 0.6055\n",
      "Epoch 2/5\n",
      "2171/2171 [==============================] - 1210s 558ms/step - loss: 1.5156 - accuracy: 0.5947 - val_loss: 1.5055 - val_accuracy: 0.5985\n",
      "Epoch 3/5\n",
      "2171/2171 [==============================] - 1210s 558ms/step - loss: 1.5145 - accuracy: 0.5964 - val_loss: 1.5036 - val_accuracy: 0.5985\n",
      "Epoch 4/5\n",
      "2171/2171 [==============================] - 1210s 557ms/step - loss: 1.5190 - accuracy: 0.5932 - val_loss: 1.5034 - val_accuracy: 0.5985\n",
      "Epoch 5/5\n",
      "2171/2171 [==============================] - 1210s 557ms/step - loss: 1.5159 - accuracy: 0.5941 - val_loss: 1.5046 - val_accuracy: 0.5985\n",
      "F1 SCORE:  0.07414082576666095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46989"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_in = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "lstm_val = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "X_forreal = np.reshape(X_forreal, (X_forreal.shape[0], X_forreal.shape[1], 1))\n",
    "del X_train, X_valid\n",
    "gc.collect()\n",
    "\n",
    "modelLSTM = trainModel(LSTMModel(), lstm_in, y_train, lstm_val, y_valid)\n",
    "\n",
    "generate_csv(modelLSTM, X_forreal, \"lstm\")\n",
    "del X_forreal, y_uniques, X_test, y_test, modelLSTM\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dqBB2FO0lS7W"
   },
   "source": [
    "# This the training of the BERT-based Model - **bertholomew()**\n",
    "\n",
    "- Data for BERT model is different from the ones used for the models above. BERT model works with strings directly, so new data needs to be loaded first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZRdKysbmDUA",
    "outputId": "c270e5ea-4fd7-41d8-a602-30b0006a4429"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X_forreal, y, y_uniques = prepare_bert_data()\n",
    "#del data, test_data #SEE IF THIS CAUSES ISSUE\n",
    "#gc.collect()\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_valid, X_train = X_train_full[:4000], X_train_full[4000:]\n",
    "y_valid, y_train = y_train_full[:4000], y_train_full[4000:]\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WvCpE14AehaA"
   },
   "outputs": [],
   "source": [
    "tfhub_handle_encoder ='https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3'\n",
    "tfhub_handle_preprocess = 'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A1-_EtnkmAzX",
    "outputId": "fc107bd3-017a-4e03-e187-a270a873ca6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2171/2171 [==============================] - 1949s 892ms/step - loss: 1.1032 - accuracy: 0.6733 - val_loss: 0.2573 - val_accuracy: 0.9365\n",
      "Epoch 2/5\n",
      "2171/2171 [==============================] - 1934s 891ms/step - loss: 0.2417 - accuracy: 0.9391 - val_loss: 0.1377 - val_accuracy: 0.9660\n",
      "Epoch 3/5\n",
      "2171/2171 [==============================] - 1933s 891ms/step - loss: 0.1439 - accuracy: 0.9651 - val_loss: 0.1170 - val_accuracy: 0.9715\n",
      "Epoch 4/5\n",
      "2171/2171 [==============================] - 1935s 891ms/step - loss: 0.1119 - accuracy: 0.9720 - val_loss: 0.1160 - val_accuracy: 0.9735\n",
      "Epoch 5/5\n",
      "2171/2171 [==============================] - 1910s 880ms/step - loss: 0.0940 - accuracy: 0.9766 - val_loss: 0.0896 - val_accuracy: 0.9780\n",
      "F1 SCORE:  0.8848392322463072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelBert = trainModel(bertholomew(), X_train, y_train, X_valid, y_valid)\n",
    "generate_csv(modelBert, X_forreal, \"bert\")\n",
    "del modelBert, X_train, y_train, X_valid,\n",
    "y_valid, X_train_full, X_test, y_train_full, y_test,\n",
    "X_forreal, y_uniques\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sFFeDR4hnQbu"
   },
   "source": [
    "# Word2Vec based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "z2SBXWIfuSnD"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dot, Embedding, Flatten\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bAAd_6Zkl7GP"
   },
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "VyRSSjcUou7d"
   },
   "outputs": [],
   "source": [
    "  vocab_size = 6626\n",
    "  embedding_dim = 100\n",
    "  num_ns = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "91mSOJXIlcFd"
   },
   "outputs": [],
   "source": [
    "# Get everything except what we want to predict\n",
    "X = np.array(data['data'])\n",
    "X_forreal = np.array(test_data['data'])\n",
    "# Column we want to predict\n",
    "y_uniques, y = label_encode(np.array(data['label']))\n",
    "\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Set output_sequence_length length to pad all samples to same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(X).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "\n",
    "vectorize_layer.adapt(text_ds.batch(1024))\n",
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "\n",
    "X = list(text_vector_ds.as_numpy_iterator())\n",
    "X = np.array(X)\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_valid, X_train = X_train_full[:4000], X_train_full[4000:]\n",
    "y_valid, y_train = y_train_full[:4000], y_train_full[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "nWkV6_7_mvBa"
   },
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for vocab_size tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "MlmEoJ5AqHic"
   },
   "outputs": [],
   "source": [
    "text_ds = tf.data.Dataset.from_tensor_slices(X_forreal).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "vectorize_layer.adapt(text_ds.batch(1024))\n",
    "\n",
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n",
    "\n",
    "X_forreal = list(text_vector_ds.as_numpy_iterator())\n",
    "\n",
    "X_forreal = np.array(X_forreal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AGDXmHXm3G2",
    "outputId": "e0909c7a-02f3-4d73-cab4-75f38bee75c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91826/91826 [01:09<00:00, 1324.81it/s]\n"
     ]
    }
   ],
   "source": [
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=X,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "ZeUExYMgpYF8"
   },
   "outputs": [],
   "source": [
    "class Word2Vec(keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = keras.layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = keras.layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "    self.dots = keras.layers.Dot(axes=(3, 2))\n",
    "    self.flatten = keras.layers.Flatten()\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    we = self.target_embedding(target)\n",
    "    ce = self.context_embedding(context)\n",
    "    dots = self.dots([ce, we])\n",
    "    return self.flatten(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Fjw_4-8RpZUF"
   },
   "outputs": [],
   "source": [
    "def custom_loss(x_logit, y_true):\n",
    "      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TeLh8p2mpaxn",
    "outputId": "d5db2c8e-eb2a-4f28-e280-3c5de313e80d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 3s 22ms/step - loss: 1.5881 - accuracy: 0.4238\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 1.2699 - accuracy: 0.6838\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.9539 - accuracy: 0.7214\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.7839 - accuracy: 0.7641\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.6726 - accuracy: 0.7940\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 2s 16ms/step - loss: 0.5922 - accuracy: 0.8166\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.5309 - accuracy: 0.8342\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.4825 - accuracy: 0.8478\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.4432 - accuracy: 0.8589\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 2s 15ms/step - loss: 0.4105 - accuracy: 0.8690\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 128\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n",
    "\n",
    "word2vec.fit(dataset, epochs=10, callbacks=[tensorboard_callback])\n",
    "\n",
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3Q52oDdqA4E",
    "outputId": "f3a2e740-006d-410e-f8c5-8c575e0db7ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "272/272 [==============================] - 9s 14ms/step - loss: 1.9494 - accuracy: 0.5484 - val_loss: 1.4719 - val_accuracy: 0.5985\n",
      "Epoch 2/10\n",
      "272/272 [==============================] - 3s 10ms/step - loss: 1.4433 - accuracy: 0.5954 - val_loss: 1.2715 - val_accuracy: 0.5985\n",
      "Epoch 3/10\n",
      "272/272 [==============================] - 3s 10ms/step - loss: 1.2408 - accuracy: 0.5915 - val_loss: 1.1049 - val_accuracy: 0.5985\n",
      "Epoch 4/10\n",
      "272/272 [==============================] - 3s 10ms/step - loss: 1.1017 - accuracy: 0.5986 - val_loss: 1.0462 - val_accuracy: 0.6298\n",
      "Epoch 5/10\n",
      "272/272 [==============================] - 3s 10ms/step - loss: 1.0526 - accuracy: 0.6311 - val_loss: 1.0141 - val_accuracy: 0.6398\n",
      "Epoch 6/10\n",
      "272/272 [==============================] - 3s 10ms/step - loss: 1.0263 - accuracy: 0.6354 - val_loss: 0.9918 - val_accuracy: 0.6503\n",
      "Epoch 7/10\n",
      "272/272 [==============================] - 3s 10ms/step - loss: 1.0071 - accuracy: 0.6490 - val_loss: 0.9748 - val_accuracy: 0.6720\n",
      "Epoch 8/10\n",
      "272/272 [==============================] - 3s 10ms/step - loss: 0.9867 - accuracy: 0.6717 - val_loss: 0.9607 - val_accuracy: 0.6860\n",
      "Epoch 9/10\n",
      "272/272 [==============================] - 3s 10ms/step - loss: 0.9822 - accuracy: 0.6829 - val_loss: 0.9470 - val_accuracy: 0.6980\n",
      "Epoch 10/10\n",
      "272/272 [==============================] - 3s 10ms/step - loss: 0.9664 - accuracy: 0.6897 - val_loss: 0.9333 - val_accuracy: 0.6980\n"
     ]
    }
   ],
   "source": [
    "training = model.fit(x = X_train, y=y_train, batch_size=256, \n",
    "                     epochs=10, shuffle=True, verbose=1, \n",
    "                     validation_data=(X_valid, y_valid))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.empty((0))\n",
    "y_pred_final = []\n",
    "y_test_final = []\n",
    "for element in y_pred:\n",
    "  y_pred_classes = np.append(y_pred_classes, np.where(element == np.amax(element))[0][0])\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "  y_pred_final.append(y_uniques[y_pred_classes[i].astype(np.int64)])\n",
    "  y_test_final.append(y_uniques[y_test[i].astype(np.int64)])\n",
    "  \n",
    "y_pred_final = np.array(y_pred_final)\n",
    "y_test_final = np.array(y_test_final)\n",
    "print(\"F1 SCORE: \", f1_score(y_test_final, y_pred_final, average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "DNPv9AHxqQ9F"
   },
   "outputs": [],
   "source": [
    "generate_csv(model, X_forreal, \"word2vec\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CS985Tidy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
