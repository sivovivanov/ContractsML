{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "explicit-inspiration",
   "metadata": {},
   "source": [
    "# CS985 Deep Learning Group L\n",
    "## Binary Classification Problem\n",
    "<p> Ian Richardson 202074007 </p>\n",
    "<p> Fraser Bayne 202053049 </p>\n",
    "<p> Slav Ivanov 201645797 </p>\n",
    "<p> Lora Kiosseva 202082329 </p>\n",
    "\n",
    "---\n",
    "In this document, binary classification of the provided dataset was run on <b>4 different models: </b> \n",
    "- A Sci-Kit Learn standard model (One verses One)\n",
    "- A baseline 3 layer dense Neural Network\n",
    "- A deeper LSTM based neural net \n",
    "- The Wide and Deep model from the book\n",
    "\n",
    "Each model runs its training separately then prints out a CSV (to the ./csv/ directory) of its predictions on the test dataset in the correct format for the Kaggle submission. The best performing models were the Baseline NN and the LSTM, producing the best Kaggle scores that we got of ~0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-teaching",
   "metadata": {},
   "source": [
    "# Method\n",
    "\n",
    "After the data is read in from the CSV files provided we are dropping the columns from the dataset that we do not need such as \"user\", \"session\" and \"query\", as they do not hold information useful for predicting the relevancy of the document, only other semantic stuff. \n",
    "\n",
    "After this, the columns which contain categorical data are one-hot encode, this way we change its type from strings to number data that can be used within the models and remove any relationships between these categories that should be unrelated. \n",
    "\n",
    "The data is split into X and Y data for use in the models and a validation set of X and Y data split off for testing the accuracy of the models. \n",
    "\n",
    "Random Over/Under samplers are used to augment the data to give us more to train from while keeping the proportions of the data the same. \n",
    "\n",
    "The Wide and Deep model requires 2 X inputs, so copies of the data are created and the X data is cut vertically into 2 overlapping sections for use in this model. \n",
    "\n",
    "---\n",
    "\n",
    "## Standard model - function: **ovo()**\n",
    "The baseline standard model chosen for this was a One Verses One classifier from the Scikit-Learn library. This model was chosen for is performance on binary classification tasks. \n",
    "\n",
    "## Baseline NN - class: **BaselineNN()**\n",
    "A baseline dense neural network, consisting of 3 hidden layers of 300 neurons each, each layers activation layer is ReLU, followed by the output layer of 1 sigmoid activated neuron. \n",
    "\n",
    "## LSTM Model - class: **LSTMModel()**\n",
    "A much deeper LSTM network was created with 8 hidden layers, 3 LSTM layers with 3 dropout layers and a dense layer at the end, followed by the same 1 neuron output layer. \n",
    "\n",
    "## WideAndDeep Model: class: **WideAndDeepModel()**\n",
    "The model WideAndDeep from the book \"Hands on Machine Learning\" [1], this model will take in 2 sets of X inputs to allow for more possible relationships to be modeled and was chosen due to the complex and multi-dimential nature of the data. \n",
    "\n",
    "## Training - function: **trainModel()**\n",
    "This is a function that will take in a specified model and run the fit() function of that model to a set of data. This made running the models much easier as all we had to do was set up the data and pass it in as a parameter to this function to train the neural net type models. This function also will print out checkpoint files, useful for loading in the weights when you would not have time to run training for such a long time. \n",
    "\n",
    "## Predict - function: **generateCSV**\n",
    "Finally, this function will take the trained model and get a prediction for the test dataset, writting it out to a CSV that can be submitted to Kaggle. The predictions were rounded as some models used sigmoid activations so all values between 0 and 1 could be written out, if the value is over 0.5 we would say that it predicted closer to a 1 so that value is rounded up. Each model writes out its raw predictions as a print before rounding to show what was predicted. \n",
    "\n",
    "\n",
    "---\n",
    "## Testing\n",
    "\n",
    "Through experimentation with the models' hyperparameters, different values produced different results. THe results gathered did not improve when changes were made, save from increaing the training times. \n",
    "\n",
    "## Performance\n",
    "This configuration runs in relitivly quick time, with the slowest model being the LSTM taking ~20sec per epoch on a GPU. The OVO classifier would run considerably slower the more rows of data given, so only the first 2000 rows of training data was given to that model to complete its run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "mathematical-orleans",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "\n",
    "from skopt import gp_minimize, callbacks, load\n",
    "from skopt.utils import cook_initial_point_generator\n",
    "from skopt.space import Real\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "insured-round",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ian\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('dataset/train.csv')\n",
    "test_data = pd.read_csv('dataset/test.csv')\n",
    "id_column = np.array(test_data[\"Id\"]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "wound-graphics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns we can drop\n",
    "data = data.drop(columns=['user', 'session', 'query', 'timestamp',\n",
    "                          'month', 'day', 'hour', 'cpvs'])\n",
    "\n",
    "test_data = test_data.drop(columns=['user', 'session', 'query', 'timestamp',\n",
    "                          'month', 'day', 'hour', 'cpvs', 'Id'])\n",
    "\n",
    "data['nature'] = data['nature'].fillna('none')\n",
    "test_data['nature'] = test_data['nature'].fillna('none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "careful-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(df, column):\n",
    "    encoder = OneHotEncoder()\n",
    "    genre_encoded, genre_categories = df[column].factorize()\n",
    "    genre_1hot = encoder.fit_transform(genre_encoded.reshape(-1,1))\n",
    "    enc_data = pd.DataFrame(genre_1hot.toarray())\n",
    "    enc_data.columns = genre_categories\n",
    "    enc_data.index = df.index\n",
    "    df = df.join(enc_data)\n",
    "    df = df.drop(columns=[column])\n",
    "    return df\n",
    "\n",
    "# Columns we want to one-hot encode\n",
    "categorical_columns = ['search', 'source', 'type', 'nature']\n",
    "for column in categorical_columns:\n",
    "    data = one_hot_encode(data, column)\n",
    "    test_data = one_hot_encode(test_data, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sporting-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we want to predict\n",
    "predict = 'psrel'\n",
    "\n",
    "# Get everything except what we want to predict\n",
    "X = np.array(data.drop([predict], 1)).astype(np.float64)\n",
    "X_forreal = np.array(test_data).astype(np.float64)\n",
    "# Column we want to predict\n",
    "y = np.array(data[predict]).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "informal-parker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33000, 30) (33000,)\n",
      "(62076, 30) (62076,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)\n",
    "oversample = RandomOverSampler(sampling_strategy='auto')\n",
    "undersample = RandomUnderSampler(sampling_strategy='auto')\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "variable-recipient",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_valid, X_train = X_train_full[:4000], X_train_full[4000:]\n",
    "y_valid, y_train = y_train_full[:4000], y_train_full[4000:]\n",
    "\n",
    "X_train_wide, X_train_deep = X_train[:,:22], X_train[:,7:]\n",
    "X_valid_wide, X_valid_deep = X_valid[:,:22], X_valid[:,7:]\n",
    "X_test_wide, X_test_deep = X_test[:,:22], X_test[:,7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sudden-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ovo():\n",
    "    cutoff = 2000\n",
    "    model = OneVsOneClassifier(SVC(kernel=\"poly\", degree=2, C=1.1, gamma=0.3,\n",
    "                                   probability=True, decision_function_shape = \"ovo\",\n",
    "                                   random_state=42))\n",
    "    model.fit(scale(X_train_full[:cutoff, :]), y_train_full[:cutoff])\n",
    "    y_pred = model.predict(scale(X_test[:cutoff, :]))\n",
    "    print(\"> One Versus One Classifier\", model.score(scale(X_test[:cutoff, :]), y_test[:cutoff]),\n",
    "          \"- f1\", f1_score(y_test[:cutoff], y_pred, average=\"weighted\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "written-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNN(keras.models.Model):\n",
    "    def __init__(self, units=300, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden3 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        hidden1 = self.hidden1(inputs)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        hidden3 = self.hidden3(hidden2)\n",
    "        main_output = self.main_output(hidden3)\n",
    "        return main_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "handled-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(keras.models.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.LSTM(256, return_sequences=True)\n",
    "        self.hidden2 = keras.layers.Dropout(0.3)\n",
    "        self.hidden3 = keras.layers.LSTM(512, return_sequences=True)\n",
    "        self.hidden4 = keras.layers.Dropout(0.3)\n",
    "        self.hidden5 = keras.layers.LSTM(256)\n",
    "        self.hidden6 = keras.layers.Dense(256)\n",
    "        self.hidden7 = keras.layers.Dropout(0.3)\n",
    "        self.main_output = keras.layers.Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        hidden1 = self.hidden1(inputs)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        hidden3 = self.hidden3(hidden2)\n",
    "        hidden4 = self.hidden4(hidden3)\n",
    "        hidden5 = self.hidden5(hidden4)\n",
    "        hidden6 = self.hidden6(hidden5)\n",
    "        hidden7 = self.hidden7(hidden6)\n",
    "        main_output = self.main_output(hidden7)\n",
    "        return main_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "statewide-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    def __init__(self, units=300, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        return main_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "constant-milton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWeights(model):\n",
    "    weightsName = \"L0.0473-B32-E3974.hdf5\"\n",
    "    print(\"loading weights\", weightsName)\n",
    "    model.load_weights(weightsName)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "compound-amount",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, xm_train, ym_train, xm_val, ym_val):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[\"binary_accuracy\"])\n",
    "    b_size = 32\n",
    "    eps = 5\n",
    "    filepath = \"weight-gen/L{loss:.4f}-B\" + str(b_size) + \"-E{epoch:02d}.hdf5\" # destination and name of saved checkpoint files\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(filepath,\n",
    "                                                 monitor=\"loss\",\n",
    "                                                 verbose=0,\n",
    "                                                 save_best_only=True,\n",
    "                                                 mode=\"min\")\n",
    "    model.fit(xm_train, ym_train, batch_size=b_size, epochs=eps, callbacks=[checkpoint], validation_data=(xm_val, ym_val))\n",
    "    y_pred = np.around(model.predict(xm_val))\n",
    "    print(\"F1 SCORE: \", f1_score(ym_val, y_pred))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fixed-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateCSV(model, x, name):\n",
    "    y_forreal = model.predict(x)\n",
    "    print(y_forreal)\n",
    "    y_forreal = np.around(y_forreal)\n",
    "\n",
    "    if(name == \"sklearnovo\"):\n",
    "        y_forreal = np.reshape(y_forreal, (-1, 1))\n",
    "        print(y_forreal)\n",
    "    \n",
    "    csv = np.concatenate((id_column, y_forreal), axis=1).astype(np.int32)\n",
    "    csv = np.vstack((np.array([\"Id\", predict]), csv))\n",
    "    np.savetxt(\"./csv/\" + name + \".csv\", csv, fmt='%s', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-dragon",
   "metadata": {},
   "source": [
    "# Results and Discussion\n",
    "- **(Accuracy: 0.666 - f1: 0.664 - Kaggle: 0.07381)** - ovo() \n",
    "- **(Accuracy: 0.724 - f1: 0.730 - Kaggle: 0.07625)** - BaselineNN()\n",
    "- **(Accuracy: 0.774 - f1: 0.774 - Kaggle: 0.10526)** - LSTMModel() \n",
    "- **(Accuracy: 0.650 - f1: 0.671 - Kaggle: 0.07334)** - WideAndDeepModel()\n",
    "\n",
    "---\n",
    "\n",
    "The baseline and LSTM models perform the best, both on Kaggle and on our validation set, the LSTM getting our highest score on Kaggle 0.105 and with longer training would perform even better. the OVO and WideAndDeep models do not perform as well, both converge fast and get lower scores on Kaggle and the more training we give it it does not improve. \n",
    "\n",
    "The features selected to be most important to us are:\n",
    "\n",
    "- search\n",
    "- source\n",
    "- type\n",
    "- nature\n",
    "\n",
    "The longer the model runs for and lower the loss value gets does not neccessarily correlate to Kaggle score performance, so sometimes running for many more epochs would not improve our scores and the best score generated was from the LSTM model with only 50 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-material",
   "metadata": {},
   "source": [
    "# Summary and Recommendation\n",
    "\n",
    "The system build can support the addition of more models as well as the models created in this are modular and could be used elsewhere in other systems for other problems. With the irrelivant columns dropped and categorical columns one-hot encoded, running each model with the data was much more straightforward and achieved better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "major-wright",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow, Aurélien Géron **(For the WideAndDeep Model)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-console",
   "metadata": {},
   "source": [
    "# Training and Validating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-there",
   "metadata": {},
   "source": [
    "## Training of the standard ML model - ovo()\n",
    "- This only trains on the first 2000 rows of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "worth-cuisine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> One Versus One Classifier 0.666 - f1 0.6640419615285641\n"
     ]
    }
   ],
   "source": [
    "modelSklearn = ovo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-oxygen",
   "metadata": {},
   "source": [
    "## Training of the baseline NN model - BaselineNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "utility-click",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1427/1427 [==============================] - 12s 4ms/step - loss: 0.8752 - binary_accuracy: 0.6260 - val_loss: 0.5742 - val_binary_accuracy: 0.6743\n",
      "Epoch 2/5\n",
      "1427/1427 [==============================] - 5s 3ms/step - loss: 0.5656 - binary_accuracy: 0.6917 - val_loss: 0.5415 - val_binary_accuracy: 0.7007\n",
      "Epoch 3/5\n",
      "1427/1427 [==============================] - 5s 3ms/step - loss: 0.5382 - binary_accuracy: 0.7082 - val_loss: 0.5197 - val_binary_accuracy: 0.7235\n",
      "Epoch 4/5\n",
      "1427/1427 [==============================] - 5s 3ms/step - loss: 0.5083 - binary_accuracy: 0.7305 - val_loss: 0.5091 - val_binary_accuracy: 0.7303\n",
      "Epoch 5/5\n",
      "1427/1427 [==============================] - 5s 3ms/step - loss: 0.4919 - binary_accuracy: 0.7412 - val_loss: 0.5062 - val_binary_accuracy: 0.7240\n",
      "F1 SCORE:  0.7309941520467835\n"
     ]
    }
   ],
   "source": [
    "modelBase = trainModel(BaselineNN(), X_train, y_train, X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-sheep",
   "metadata": {},
   "source": [
    "## Training of the LSTM Model - LSTMModel()\n",
    "- Data needs to first be reshaped to be used by this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "renewable-survivor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1427/1427 [==============================] - 39s 19ms/step - loss: 0.6837 - binary_accuracy: 0.5530 - val_loss: 0.6563 - val_binary_accuracy: 0.6047\n",
      "Epoch 2/5\n",
      "1427/1427 [==============================] - 26s 18ms/step - loss: 0.6312 - binary_accuracy: 0.6252 - val_loss: 0.5469 - val_binary_accuracy: 0.7237\n",
      "Epoch 3/5\n",
      "1427/1427 [==============================] - 26s 18ms/step - loss: 0.5512 - binary_accuracy: 0.7270 - val_loss: 0.5307 - val_binary_accuracy: 0.7333\n",
      "Epoch 4/5\n",
      "1427/1427 [==============================] - 26s 18ms/step - loss: 0.5276 - binary_accuracy: 0.7403 - val_loss: 0.5110 - val_binary_accuracy: 0.7473\n",
      "Epoch 5/5\n",
      "1427/1427 [==============================] - 26s 18ms/step - loss: 0.5125 - binary_accuracy: 0.7499 - val_loss: 0.5044 - val_binary_accuracy: 0.7598\n",
      "F1 SCORE:  0.7736160188457009\n"
     ]
    }
   ],
   "source": [
    "lstm_in = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "lstm_val = np.reshape(X_valid, (X_valid.shape[0], X_valid.shape[1], 1))\n",
    "modelLSTM = trainModel(LSTMModel(), lstm_in, y_train, lstm_val, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-mileage",
   "metadata": {},
   "source": [
    "## Training of the WideAndDeep Model - WideAndDeepModel()\n",
    "- This model will require 2 X inputs, seporated earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sweet-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "388/388 [==============================] - 2s 3ms/step - loss: 0.7546 - binary_accuracy: 0.6160 - val_loss: 0.6355 - val_binary_accuracy: 0.6538\n",
      "Epoch 2/5\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 0.6365 - binary_accuracy: 0.6459 - val_loss: 0.6420 - val_binary_accuracy: 0.6455\n",
      "Epoch 3/5\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 0.6382 - binary_accuracy: 0.6518 - val_loss: 0.6443 - val_binary_accuracy: 0.6357\n",
      "Epoch 4/5\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 0.6317 - binary_accuracy: 0.6532 - val_loss: 0.6311 - val_binary_accuracy: 0.6568\n",
      "Epoch 5/5\n",
      "388/388 [==============================] - 1s 3ms/step - loss: 0.6283 - binary_accuracy: 0.6578 - val_loss: 0.6294 - val_binary_accuracy: 0.6497\n",
      "F1 SCORE:  0.670585469080649\n"
     ]
    }
   ],
   "source": [
    "modelWAD = trainModel(WideAndDeepModel(), [X_test_wide, X_test_deep], y_test, [X_valid_wide, X_valid_deep], y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-electricity",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "- Each model is passed into the generateCSV() function to get its prediction on the test data set and write it out to a CSV file for submission on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "specialized-ottawa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 1. 1. 1.]\n",
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "generateCSV(modelSklearn, X_forreal, \"sklearnovo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "extra-surfing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42983472]\n",
      " [0.2509963 ]\n",
      " [0.66495335]\n",
      " ...\n",
      " [0.42983472]\n",
      " [0.59225297]\n",
      " [0.6720405 ]]\n"
     ]
    }
   ],
   "source": [
    "generateCSV(modelBase, X_forreal, \"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "primary-destruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52756566]\n",
      " [0.2173662 ]\n",
      " [0.3529291 ]\n",
      " ...\n",
      " [0.71345186]\n",
      " [0.4166437 ]\n",
      " [0.3464281 ]]\n"
     ]
    }
   ],
   "source": [
    "lstm_forreal = np.reshape(X_forreal, (X_forreal.shape[0], X_forreal.shape[1], 1))\n",
    "generateCSV(modelLSTM, lstm_forreal, \"lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "awful-simpson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3242547 ]\n",
      " [0.32384267]\n",
      " [0.30399007]\n",
      " ...\n",
      " [0.02773135]\n",
      " [0.53214264]\n",
      " [0.50087696]]\n"
     ]
    }
   ],
   "source": [
    "X_forreal_wide, X_forreal_deep = X_forreal[:,:22], X_forreal[:,7:]\n",
    "generateCSV(modelWAD, [X_forreal_wide, X_forreal_deep], \"wad\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
